{"cells":[{"cell_type":"markdown","source":["Maggy  is an open-source framework that simplifies writing and maintaining distributed machine learning programs.\nBy encapsulating your training logic in a function,\nthe same code can be run unchanged with Python on your laptop or distributed using PySpark for hyperparameter tuning, \ndata-parallel training, or model-parallel training. \nWith the arrival of GPU support in Spark 3.0, \nPySpark can be now used to orchestrate distributed deep learning applications in TensorFlow and PySpark.  \nWe are pleased to announce we have now added support for Maggy on Databricks, \nso training machine learning models with many workers should be as easy as running Python programs on your laptop."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fb3f723-2c1d-4d47-8584-2976ca232d5a"}}},{"cell_type":"markdown","source":["### 0. Spark Session\n\nFirst, make sure you have a running Spark Session/Context available."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f1528e6-b17f-440c-9380-97b0a27d2eaf"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9602de1-25e0-4ae7-88ee-c9182ab01116"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Make sure you have the right tensorflow version."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a3f0602-b2c7-4799-a11f-7f4e59f9425f"}}},{"cell_type":"code","source":["%pip install tensorflow-cpu==2.4.1\nimport tensorflow as tf"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75c5f372-550d-4f01-be80-70d68f1df449"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 1. Model definition\n\nLet's define the model we want to train. The layers of the model have to be defined in the \\_\\_init__ function.\n\nDo not instantiate the class, otherwise you won't be able to use Maggy."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dffedc6-f4a4-4952-b171-c93ebbb51316"}}},{"cell_type":"code","source":["from tensorflow import keras \nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\n# you can use keras.Sequential(), you just need to override it \n# on a custom class and define the layers in __init__()\nclass NeuralNetwork(Sequential):\n        \n    def __init__(self, nl=4):\n        \n        super().__init__()\n        self.add(Dense(10,input_shape=(None,4),activation='tanh'))\n        if nl >= 4:\n          for i in range(0, nl-2):\n            self.add(Dense(8,activation='tanh'))\n        self.add(Dense(3,activation='softmax'))\n\nmodel = NeuralNetwork"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d6f1fbb-6a2c-4d5d-9de3-2ca2541ec65e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 2. Dataset creation\n\nIn this example, we are using the iris dataset. Let's download the dataset from https://www.kaggle.com/uciml/iris and upload it on your Databricks profile.\n\nYou can process the dataset in the notebook and pass it to the configuration classes, or process it during the experiment.\nIn order to do that you have to wrap the processing logic in a function and pass it to the training configuration (this step is currently supported only by TfDistributedConfig).\n\nYou need to change the dataset path is correct."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdcb2817-3b0e-4f03-87f4-3e066384ed82"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/tables/iris_train-2.csv\"))"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0bbdf75-e3d8-484d-baa8-16a7afd382eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["train_set_path = \"dbfs:/FileStore/tables/iris_train-2.csv\"\ntest_set_path = \"dbfs:/FileStore/tables/iris_test-1.csv\"\n\ntrain_set = spark.read.format(\"csv\").option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\").load(train_set_path).drop('_c0')\n\ntest_set = spark.read.format(\"csv\").option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\").load(test_set_path).drop('_c0')\n\nraw_train_set = train_set.toPandas().values\nraw_test_set = test_set.toPandas().values"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c15ffc96-470d-4bb0-8e5a-1b5e4277c806"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can also wrap the data processing in a function and pass it to the training configuration, as we'll see later."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1ce270e-7db8-4137-acef-ac8bf583ff19"}}},{"cell_type":"code","source":["def process_data(train_set, test_set):\n    \n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.model_selection import train_test_split\n    \n    X_train = train_set[:,0:4]\n    y_train = train_set[:,4:]\n    X_test = test_set[:,0:4]\n    y_test = test_set[:,4:]\n\n    return (X_train, y_train), (X_test, y_test)\n  \ntrain_set, test_set = process_data(raw_train_set, raw_test_set)"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13cf5223-1f3c-4c97-8b67-aad68fd09b6a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3. Wrap the training logics.\n\nThe programming model is that you wrap the code containing the logics of your experiment in a function.\n\nFor HPO, we have to define a function that has the HPs to be optimized as parameters. Inside the function we simply put\nthe training logic as we were training our model in a single machine using Tensorflow."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37370351-85b5-48c7-803a-d26379d332f9"}}},{"cell_type":"code","source":["def hpo_function(number_layers, reporter):\n  \n  model = NeuralNetwork(nl=number_layers)\n  model.build()\n  \n  #fitting the model and predicting\n  model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n  train_input, test_input = process_data(raw_train_set, raw_test_set)\n\n  train_batch_size = 75\n  test_batch_size = 15\n  epochs = 10\n  \n  model.fit(x=train_input[0], y=train_input[1],\n            batch_size=train_batch_size,\n            epochs=epochs,\n            verbose=1)\n\n  score = model.evaluate(x=test_input[0], y=test_input[1], batch_size=test_batch_size, verbose=1)\n                         \n  print(f'Test loss: {score[0]}')\n  print(f'Test accuracy: {score[1]}')\n\n  return score[1]"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e403a2e-b7e5-41ff-bc7c-fb8f9c3ad532"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We do the same for the training function, this time passing the model, train_set, test_set and hparams."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa4b77cd-ec40-4acb-8648-ab12d3896195"}}},{"cell_type":"code","source":["def training_function(model, train_set, test_set, hparams):\n    \n    model = model(nl=hparams['number_layers'])\n    model.build()\n    #fitting the model and predicting\n\n    model.compile(Adam(lr=hparams['learning_rate']),'categorical_crossentropy',metrics=['accuracy'])\n    \n    #raise ValueError(list(train_set.as_numpy_iterator()))\n\n    model.fit(train_set,epochs=hparams['epochs'])\n\n    accuracy = model.evaluate(test_set)\n\n    return accuracy"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d922db38-2c73-4158-a686-b3fa522768fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the next step we have to create a configuration instance for Maggy. Since in this example we are using Maggy for hyperparameter optimization and distributed training using TensorFlow, we will use OptimizationConfig and TfDistributedConfig.\n\n### 4. Configure and run distributed HPO\n\n\nOptimizationConfig contains the information about the hyperparameter optimization.\nWe need to define a Searchspace class that contains the hyperparameters we want to get optimized. In this example we want to search for the optimal number of layers of the neural network from 2 to 8 layers.\n\nOptimizationConfig the following parameters:\n* num_trials: Controls how many separate runs are conducted during the hp search.\n* optimizer: Optimizer type for searching the hp searchspace.\n* searchspace: A Searchspace object configuring the names, types and ranges of hps.\n* optimization_key: Name of the metric to use for hp search evaluation.\n* direction: Direction of optimization.\n* es_interval: Early stopping polling frequency during an experiment run.\n* es_min: Minimum number of experiments to conduct before starting the early stopping mechanism. Useful to establish a baseline for performance estimates.\n* es_policy: Early stopping policy which formulates a rule for triggering aborts.\n* name: Experiment name.\n* description: A description of the experiment.\n* hb_interval: Heartbeat interval with which the server is polling.\n* fixed_hp: Hyperparamets not to be tuned."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef55d682-0a88-4113-a1f7-79311a783cf6"}}},{"cell_type":"code","source":["from maggy.experiment_config import OptimizationConfig\nfrom maggy import Searchspace\n\n# The searchspace can be instantiated with parameters\nsp = Searchspace(number_layers=('INTEGER', [2, 8]))\n\nhpo_config = OptimizationConfig(num_trials=4, optimizer=\"randomsearch\", searchspace=sp, direction=\"max\", es_interval=1, es_min=5, name=\"hp_tuning_test\")"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f53c2b69-a6d9-4822-a2ec-80cd9a189656"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Our HPO function and configuration class are now ready, so we can go on and run the HPO experiment. In order to do that, we run the lagom function, passing our training function and the configuration object we instantiated during the last step.\nLagom is a swedish word meaning \"just the right amount\"."],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22c0f078-416f-40e2-863b-0b01bc71b60c"}}},{"cell_type":"code","source":["from maggy import experiment\n\nresult = experiment.lagom(train_fn=hpo_function, config=hpo_config)\n\nprint(result)"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86ed9262-c7b1-41b8-adb7-b85b5d2fc813"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5. Configure and run distributed training\n\n\nNow it's time to run the final step of our ML program. Let's initialize the configuration class for the distributed training. First, we need to define our hyperparameters, we want to take the best hyperparameters from the HPO."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b531eef-f446-4c06-85b5-7e258e8b96d0"}}},{"cell_type":"code","source":["#define the constructor parameters of your model\nmodel_params = {\n    #train dataset entries / num_workers\n    'train_batch_size': 75,\n    #test dataset entries / num_workers\n    'test_batch_size': 15,\n    'learning_rate': 0.04,\n    'epochs': 20,\n    'number_layers': result['best_config']['number_layers'],\n}"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"647635ae-b25b-4fc7-aa06-4f1b0ad816ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["TfDistributedConfig class contains the following parameters:\n* name: the name of the experiment.\n* module: the model to be trained (defined in the first step of this guideline).\n* train_set: the train set as a tuple (x_train, y_train) or the train set path.\n* test_set: the test set as a tuple (x_test, y_test) or the test set path.\n* process_data: the function to process the data (if needed).\n* hparams: the model and dataset parameters. In this case we also need to provide the 'train_batch_size' and the 'test_batch_size', these values represent the subset sizes of the sharded dataset. It's value is usually the dataset_size/number_workers but can change depending on your needs."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e8cf9ac-289b-4743-bbca-cd362bddedf5"}}},{"cell_type":"code","source":["from maggy.experiment_config.tf_distributed import TfDistributedConfig\n\ntraining_config = TfDistributedConfig(name=\"tf_test\", model=model, train_set=train_set, test_set=test_set, process_data=process_data, hparams = model_params)"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d2422ac-8be8-45fd-8bbf-6e37badd4cac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we are ready to launch the maggy experiment. You just need to pass 2 parameters: the training function and the configuration variable we defined in the previous steps."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88ca9e39-1510-4d3c-8ad4-db8c4949810e"}}},{"cell_type":"code","source":["experiment.lagom(training_function, training_config)"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a594363a-2bd9-41ed-ae28-770497b8b130"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":3},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"maggy-databricks-iris","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1761503122828067}},"nbformat":4,"nbformat_minor":0}
