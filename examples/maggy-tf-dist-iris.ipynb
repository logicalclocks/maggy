{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8d083e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Maggy enables you to train with Tensorflow distributed optimizers.\n",
    "Using Maggy, you have to make minimal changes in train your model in a distributed fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedffa4",
   "metadata": {},
   "source": [
    "### 0. Spark Session\n",
    "\n",
    "Make sure you have a running Spark Session/Context available.\n",
    "\n",
    "On Hopsworks, just run your notebook to start the spark application.\n",
    "\n",
    "### 1. Model definition\n",
    "\n",
    "Let's define the model we want to train. The layers of the model have to be defined in the \\_\\_init__ function.\n",
    "\n",
    "Do not instantiate the class, otherwise you won't be able to use Maggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a3d169",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>48</td><td>application_1619105773947_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1619105773947_0003/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e12_1619105773947_0003_01_000001/demo_ml_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# you can use keras.Sequential(), you just need to override it \n",
    "# on a custom class and define the layers in __init__()\n",
    "class NeuralNetwork(Sequential):\n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.add(Dense(10,input_shape=(4,),activation='tanh'))\n",
    "        self.add(Dense(8,activation='tanh'))\n",
    "        self.add(Dense(6,activation='tanh'))\n",
    "        self.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model = NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ba441",
   "metadata": {},
   "source": [
    "### 2. Dataset creation\n",
    "\n",
    "You can create the dataset here and pass it to the TfDistributedConfig, or creating it in the training function.\n",
    "\n",
    "You need to change the dataset path is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713966cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = \"hdfs:///Projects/demo_ml_meb10000/TourData/iris/train.csv\"\n",
    "test_set_path = \"hdfs:///Projects/demo_ml_meb10000/TourData/iris/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae96821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_set, test_set):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_set = pd.read_csv(train_set)\n",
    "    test_set = pd.read_csv(test_set)\n",
    "\n",
    "    X_train = train_set.iloc[1:,1:5].values\n",
    "    y_train = train_set.iloc[1:,5:].values\n",
    "    X_test = test_set.iloc[1:,1:5].values\n",
    "    y_test = test_set.iloc[1:,5:].values\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc55a64",
   "metadata": {},
   "source": [
    "### 3. Defining the training function\n",
    "\n",
    "The programming model is that you wrap the code containing the model training inside a wrapper function. Inside that wrapper function provide all imports and parts that make up your experiment.\n",
    "\n",
    "The function should return the metric that you want to optimize for. This should coincide with the metric being reported in the Keras callback (see next point).\n",
    "You can return the metric list, in this case only the loss element will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d667c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_function(model, train_set, test_set, hparams):\n",
    "    \n",
    "    model = model()\n",
    "    model.build()\n",
    "    #fitting the model and predicting\n",
    "\n",
    "    model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_set,epochs=20)\n",
    "\n",
    "    accuracy = model.evaluate(test_set)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5709472",
   "metadata": {},
   "source": [
    "### 4. Configuring the experiment\n",
    "\n",
    "In order to use maggy distributed training, we have to configure the training model, we can pass it to TfDistributedConfig.\n",
    "the model class has to be an implementation of __tf.keras.Model__.\n",
    "We can also define __train_set__, __test_set__ and eventually the __model_parameters__. __model_parameters__ is a dictionary\n",
    "containing the parameters to be used in the \\_\\_init__ function of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy.experiment_config.tf_distributed import TfDistributedConfig\n",
    "\n",
    "#define the constructor parameters of your model\n",
    "model_params = {\n",
    "    #train dataset entries / num_workers\n",
    "    'train_batch_size': 75,\n",
    "    #test dataset entries / num_workers\n",
    "    'test_batch_size': 15,\n",
    "    'nlayers':2\n",
    "}\n",
    "\n",
    "config = TfDistributedConfig(name=\"tf_test\", module=model, train_set=train_set_path, test_set=test_set_path, process_data = process_data, hparams = model_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db60782",
   "metadata": {},
   "source": [
    "### 5. Run distributed training\n",
    "\n",
    "Finally, we are ready to launch the maggy experiment. You just need to pass 2 parameters: the training function and the configuration variable we defined in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2833f8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d164a3c49014a949b0b33389aad8938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=1.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: All executors registered: True\n",
      "0: All executors registered: True\n",
      "1: Epoch 1/20\n",
      "1: \n",
      "1: \n",
      "0: Epoch 1/20\n",
      "0: \n",
      "0: \n",
      "1: Epoch 2/20\n",
      "1: Epoch 3/20\n",
      "1: Epoch 4/20\n",
      "1: Epoch 5/20\n",
      "1: Epoch 6/20\n",
      "1: Epoch 7/20\n",
      "1: Epoch 8/20\n",
      "0: Epoch 2/20\n",
      "0: Epoch 3/20\n",
      "0: Epoch 4/20\n",
      "0: Epoch 5/20\n",
      "0: Epoch 6/20\n",
      "0: Epoch 7/20\n",
      "0: Epoch 8/20\n",
      "0: Epoch 9/20\n",
      "0: Epoch 10/20\n",
      "1: Epoch 9/20\n",
      "1: Epoch 10/20\n",
      "1: Epoch 11/20\n",
      "1: Epoch 12/20\n",
      "1: Epoch 13/20\n",
      "1: Epoch 14/20\n",
      "1: Epoch 15/20\n",
      "1: Epoch 16/20\n",
      "1: Epoch 17/20\n",
      "1: Epoch 18/20\n",
      "1: Epoch 19/20\n",
      "1: Epoch 20/20\n",
      "0: Epoch 11/20\n",
      "0: Epoch 12/20\n",
      "0: Epoch 13/20\n",
      "0: Epoch 14/20\n",
      "0: Epoch 15/20\n",
      "0: Epoch 16/20\n",
      "0: Epoch 17/20\n",
      "0: Epoch 18/20\n",
      "0: Epoch 19/20\n",
      "0: Epoch 20/20\n",
      "0: \n",
      "0: \n",
      "1: \n",
      "1: \n",
      "Final average test loss: 0.207\n",
      "Finished experiment. Total run time: 0 hours, 0 minutes, 29 seconds\n",
      "{'test result': 0.20700063556432724}\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "\n",
    "experiment.lagom(training_function, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040d274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddef44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
