{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Maggy is a framework for distribution transparent machine learning experiments on Apache Spark. In this post, we introduce a new unified framework for writing core ML training logic as oblivious training functions. Maggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster. Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context.</p> <p> Maggy uses the same distribution transparent training function in all steps of the machine learning development process. </p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the <code>'spark'</code> extra, to run on Spark in local mode:</p> <pre><code>pip install maggy\n</code></pre> <p>The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment.</p> <p>Single run experiment:</p> <pre><code>def train_fn():\n    # This is your training iteration loop\n    for i in range(number_iterations):\n        ...\n        # add the maggy reporter to report the metric to be optimized\n        reporter.broadcast(metric=accuracy)\n         ...\n    # Return metric to be optimized or any metric to be logged\n    return accuracy\n\nfrom maggy import experiment\nresult = experiment.lagom(train_fn=train_fn, name='MNIST')\n</code></pre> <p>lagom is a Swedish word meaning \"just the right amount\". This is how MAggy uses your resources.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation is available at maggy.ai</p>"},{"location":"#contributing","title":"Contributing","text":"<p>There are various ways to contribute, and any contribution is welcome, please follow the CONTRIBUTING guide to get started.</p>"},{"location":"#issues","title":"Issues","text":"<p>Issues can be reported on the official GitHub repo of Maggy.</p>"},{"location":"#citation","title":"Citation","text":"<p>Please see our publications on maggy.ai to find out how to cite our work.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The development of Maggy is supported by the EU H2020 Deep Cube Project (Grant agreement ID: 101004188).</p>"},{"location":"CONTRIBUTING/","title":"How to contribute","text":"<p>Contributions are welcome! Not familiar with the codebase yet? No problem! There are many ways to contribute to open source projects: reporting bugs, helping with the documentation, spreading the word and of course, adding new features and patches.</p>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting issues","text":"<ul> <li>Describe what you expected to happen.</li> <li>If possible, include a minimal, complete, and verifiable example to help   us identify the issue. This also helps to check that the issue is not with   your own code.</li> <li>Describe what actually happened. Include the full traceback if there was an   exception.</li> <li>List your Python, Hopsworks and Maggy versions. If possible, check if this   issue is already fixed in the repository.</li> </ul>"},{"location":"CONTRIBUTING/#contributing-code","title":"Contributing Code","text":"<p>Code contributions, in the form of patches or features are welcome. In order to start developing, please follow the instructions below, to enable pre-commit and ensure style and codechecks.</p>"},{"location":"CONTRIBUTING/#python-setup","title":"Python Setup","text":"<ul> <li> <p>Fork Maggy to your GitHub account by clicking the <code>Fork</code> button.</p> </li> <li> <p>Clone your fork locally:</p> </li> </ul> <pre><code>git clone https://github.com/[username]/maggy.git\ncd maggy\n</code></pre> <ul> <li>Add the upstream repository as a remote to update later::</li> </ul> <pre><code>git remote add upstream https://github.com/logicalclocks/maggy.git\ngit fetch upstream\n</code></pre> <ul> <li>Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda:</li> </ul> <pre><code>python3 -m venv env\n. env/bin/activate\n# or \"env\\Scripts\\activate\" on Windows\n</code></pre> <p>or with conda:</p> <pre><code>conda create --name maggy python=3.8\nconda activate maggy\n</code></pre> <p>verify your python version - we are using Python 3.8:</p> <pre><code>python --version\n</code></pre> <ul> <li>Install Maggy in editable mode with development dependencies::</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ul> <li>Install pre-commit_ and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Maggy uses pre-commit to ensure code-style and code formatting through black and flake8:</li> </ul> <pre><code>pip install --user pre-commit\npre-commit install\n</code></pre> <p>Afterwards, pre-commit will run whenever you commit.</p> <ul> <li>To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line:</li> </ul> <pre><code>flake8 maggy\nblack maggy\n</code></pre>"},{"location":"CONTRIBUTING/#start-coding","title":"Start coding","text":"<ul> <li>Create a branch to identify the issue or feature you would like to work on.</li> <li>Using your favorite editor, make your changes, committing as you go.</li> <li>Follow PEP8.</li> <li>Push your commits to GitHub and create a pull request.</li> <li>Celebrate \ud83c\udf89</li> </ul>"},{"location":"blogs/","title":"Blogs","text":""},{"location":"publications/","title":"Publications","text":"<p>If you use Maggy for research, or write about Maggy please cite the following papers:</p>"},{"location":"publications/#maggy-hyperparameter-optimization","title":"Maggy Hyperparameter Optimization","text":""},{"location":"publications/#maggy-scalable-asynchronous-parallel-hyperparameter-search","title":"Maggy: Scalable Asynchronous Parallel Hyperparameter Search","text":""},{"location":"publications/#authors","title":"Authors","text":"<p>Moritz Meister, Sina Sheikholeslami, Amir H. Payberah, Vladimir Vlassov, Jim Dowling</p>"},{"location":"publications/#abstract","title":"Abstract","text":"<p>Running extensive experiments is essential for building Machine Learning (ML) models. Such experiments usually require iterative execution of many trials with varying run times. In recent years, Apache Spark has become the de-facto standard for parallel data processing in the industry, in which iterative processes are im- plemented within the bulk-synchronous parallel (BSP) execution model. The BSP approach is also being used to parallelize ML trials in Spark. However, the BSP task synchronization barriers prevent asynchronous execution of trials, which leads to a reduced number of trials that can be run on a given computational budget. In this paper, we introduce Maggy, an open-source framework based on Spark, to execute ML trials asynchronously in parallel, with the ability to early stop poorly performing trials. In the experiments, we compare Maggy with the BSP execution of parallel trials in Spark and show that on random hyperparameter search on a con- volutional neural network for the Fashion-MNIST dataset Maggy reduces the required time to execute a fixed number of trials by 33% to 58%, without any loss in the final model accuracy.</p> <p>Download Paper</p>"},{"location":"publications/#cite","title":"Cite","text":"<pre><code>@inproceedings{10.1145/3426745.3431338,\nauthor = {Meister, Moritz and Sheikholeslami, Sina and Payberah, Amir H. and Vlassov, Vladimir and Dowling, Jim},\ntitle = {Maggy: Scalable Asynchronous Parallel Hyperparameter Search},\nyear = {2020},\nisbn = {9781450381826},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3426745.3431338},\ndoi = {10.1145/3426745.3431338},\nbooktitle = {Proceedings of the 1st Workshop on Distributed Machine Learning},\npages = {28\u201333},\nnumpages = {6},\nkeywords = {Scalable Hyperparameter Search, Machine Learning, Asynchronous Hyperparameter Optimization},\nlocation = {Barcelona, Spain},\nseries = {DistributedML'20}\n}\n</code></pre>"},{"location":"publications/#oblivious-training-functions","title":"Oblivious Training Functions","text":""},{"location":"publications/#towards-distribution-transparency-for-supervised-ml-with-oblivious-training-functions","title":"Towards Distribution Transparency for Supervised ML With Oblivious Training Functions","text":""},{"location":"publications/#authors_1","title":"Authors","text":"<p>Moritz Meister, Sina Sheikholeslami, Robin Andersson, Alexandru A. Ormenisan, Jim Dowling</p>"},{"location":"publications/#abstract_1","title":"Abstract","text":"<p>Building and productionizing Machine Learning (ML) models is a process of interdependent steps of iterative code updates, including exploratory model design, hyperparameter tuning, ablation experiments, and model training. Industrial-strength ML involves doing this at scale, using many compute resources, and this requires rewriting the training code to account for distribution. The result is that moving from a single host program to a cluster hinders iterative development of the software, as iterative development would require multiple versions of the software to be maintained and kept consistent. In this paper, we introduce the distribution oblivious training function as an abstraction for ML development in Python, whereby developers can reuse the same training function when running a notebook on a laptop or performing scale-out hyperparameter search and distributed training on clusters. Programs written in our framework look like industry-standard ML programs as we factor out dependencies using best-practice programming idioms (such as functions to generate models and data batches). We believe that our approach takes a step towards unifying single-host and distributed ML development.</p> <p>Download Paper</p>"},{"location":"publications/#cite_1","title":"Cite","text":"<pre><code>@inproceedings{oblivious-mlops,\nauthor = {Meister, Moritz and Sheikholeslami, Sina and Andersson, Robin and Ormenisan, Alexandru A. and Dowling, Jim},\ntitle = {Towards Distribution Transparency for Supervised ML With Oblivious Training Functions},\nyear = {2020},\nbooktitle = {MLSys \u201920: Workshop on MLOps Systems, March 02\u201304},\nlocation = {Austin, Texas, USA}\n}\n</code></pre>"},{"location":"ablation/intro/","title":"Quick Start","text":"<p>Ablation studies have become best practice in evaluating model architectures, as they provide insights into the relative  contribution of the different architectural and regularization components to the performance of models.  An ablation study consists of several trials, where one trial could be,  e.g., removing the last convolutional layer of a CNN model, retraining the model,  and observing the resulting performance.  However, as machine learning architectures become ever deeper and data sizes keep growing,  there is an explosion in the number of different architecture combinations that need to be evaluated to understand  their relative performance. </p> <p>Maggy provides a declarative way to define ablation experiments for model architectures  and training datasets, in a way that eliminates the need for maintaining redundant copies of code for an ablation study. Furthermore, our framework enables parallel execution of ablation trials without requiring the developers to  modify their code, which leads to shorter study times and better resource utilization. </p>"},{"location":"ablation/intro/#simple-example","title":"Simple Example","text":"<p>In order to use Maggy for Ablation studies, first we need to define a model generator function. <pre><code>from maggy.ablation import AblationStudy\n\nablation_study = AblationStudy('titanic_train_dataset', training_dataset_version=1,\n                              label_name='survived')\n</code></pre></p>"},{"location":"ablation/intro/#add-the-features-to-ablate","title":"Add the features to ablate","text":"<p>We perform feature ablation by including features in our AblationStudy instance. Including a feature means that there  will be a trial where the model will be trained without that feature.  In other words, you include features in the ablation study so that they will be excluded from the training dataset.</p> <p>In this example, we have the following features in our training dataset: <pre><code>['age', 'fare', 'parch', 'pclass', 'sex', 'sibsp', 'survived']\n</code></pre></p> <p>You can include features using <code>features.include()</code> method of your AblationStudy instance,  by passing the names of the features, either separately or as a list of strings:</p> <pre><code>#include features one by one\nablation_study.features.include('pclass')\n\n# include a list of features\nlist_of_features = ['fare', 'sibsp']\nablation_study.features.include(list_of_features)\n</code></pre>"},{"location":"ablation/intro/#add-the-model-layers-to-ablate","title":"Add the model layers to ablate","text":"<p>By model ablation we mean removing the components of the model, retraining and observing the resulting performance.  Depending on which component of the model you want to ablate, we could have different types of model ablation,  but one thing they all share in common is that we should have one base model in order to compare the other  models with it. So we should define a base model generator function that returns a <code>tf.keras.Model</code>.</p> <p>Maybe the simplest type of model ablation that can be performed on a sequential deep learning model is to  remove some of its layers, so let\u2019s just do that. We call this layer ablation.  In Keras, when you are adding layers to your <code>Sequential</code> model,  you can provide a name argument with a custom name.  The Maggy ablator then uses these names to identify and remove the layers.  Then, for each trial, the ablator returns a corresponding model generator function that differs from the base  model generator in terms of its layers.</p> <p>By the way, if you do not provide a name argument while adding a layer, that layer\u2019s name will be prefixed by its  layer class name, followed by an incremental number, e.g., dense_1.</p> <p>In the following cell, we define a base model generator function that once executed will  return a <code>Sequential</code> model: <pre><code>def base_model_generator():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(64, name='my_dense_two', activation='relu'))\n    model.add(tf.keras.layers.Dense(32, name='my_dense_three', activation='relu'))\n    model.add(tf.keras.layers.Dense(32, name='my_dense_four', activation='relu'))\n    model.add(tf.keras.layers.Dense(2, name='my_dense_sigmoid', activation='sigmoid'))\n    # output layer\n    model.add(tf.keras.layers.Dense(1, activation='linear'))\n    return model\n\nablation_study.model.set_base_model_generator(base_model_generator)\n</code></pre></p> <p>Adding layers to your ablation study is easy - just pass their names to <code>model.layers.include()</code>,  or pass a list of strings of the names. Of course, these names should match the names you define in your base model  generator function: <pre><code># include some single layers in the ablation study\n\nablation_study.model.layers.include('my_dense_two', \n                                    'my_dense_three', \n                                    'my_dense_four', \n                                    'my_dense_sigmoid'\n                                    )\n</code></pre></p> <p>For smaller models, it might make sense to add the layers to the model one-by-one.  However, in many well-known neural network architectures, such as Transformers,  we have tens or hundreds of layers that sometimes come  in blocks or modules, and are usually generated using constructs like for loops.</p> <p>In Maggy, you can easily include such layers in your ablation study experiment,  using <code>model.layers.include_groups()</code> method of your AblationStudy instance.  You can either pass it a list of layers that should be regarded as a single layer group,  or provide it with a prefix argument:</p> <pre><code># add a layer group using a list\nablation_study.model.layers.include_groups(['my_dense_two', 'my_dense_four'])\n\n# add a layer group using a prefix\nablation_study.model.layers.include_groups(prefix='my_dense')\n</code></pre>"},{"location":"ablation/intro/#write-the-training-logic","title":"Write the training logic","text":"<p>Now the only thing you need to do is to write your training code in a Python function.  You can name this function whatever you wish, but we will refer to it as the training function.  The model_function and dataset_function used in the code are generated by the ablator per each trial,  and you should call them in your code. This is your run-of-the-mill TensorFlow/Keras code:</p> <p><pre><code># wrap your code in a Python function\ndef training_fn(dataset_function, model_function):\n    import tensorflow as tf\n    epochs = 5\n    batch_size = 10\n    tf_dataset = dataset_function(epochs, batch_size)\n    model = model_function()\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\n\n    history = model.fit(tf_dataset, epochs=5, steps_per_epoch=30, verbose=0)\n    return float(history.history['accuracy'][-1])\n</code></pre> And, lagom! Lagom is a Swedish word that means \u201cjust the right amount\u201d, and that is how Maggy uses your resources to  for parallel trials. You should provide lagom with an ablator. So far, we have implemented the most natural ablator of  them all: LOCO, which stands for \u201cLeave One Component Out\u201d. This ablator will generate one trial per each component  included in the ablation study. However, Maggy\u2019s developer API allows you to define your own ablator,  in case you want to get creative.</p> <p>You can also set a name for your experiment so that you can keep history or track its progress in Hopsworks.</p> <p>Let\u2019s lagom our experiment!</p> <pre><code># Create a config for lagom\nfrom maggy.experiment_config import AblationConfig\nfrom maggy.experiment import experiment\n\nconfig = AblationConfig(name=\"Titanic-LOCO\", ablation_study=ablation_study, ablator=\"loco\", description=\"\", hb_interval=1)\n# launch the experiment\nresult = experiment.lagom(train_fn=training_fn, config=config)\n</code></pre>"},{"location":"dist_training/intro/","title":"Introduction","text":"<p>Distributed training is useful for big models that can't fit in a single machine and for very big datasets. There are several techniques available. As an example, the Mirrored Strategies replicate the models over the workers and  train them using splits of the data. </p> <p>With Maggy, you can train a Machine Learning model in a distributed fashion without rewriting the code of the training. Distributed Training with Maggy is available on TensorFlow and PyTorch.</p> <p>If you want to know more on how to use Maggy for Distributed Training, you can watch the presentation in the next section.</p> <p>When you are ready, you can inspect an example on TensorFlow or PyTorch.</p>"},{"location":"dist_training/intro/#maggy-distributed-model-training","title":"Maggy Distributed Model Training","text":""},{"location":"dist_training/tensorflow/","title":"Quick Start","text":"<p>Using maggy for Distributed Training works as follows:</p> <ul> <li>Optionally, define a model generator object, similarly to what is done for Ablation Studies. <pre><code>class MyModel(tf.keras.Model):\n\n    def __init__(self, ...):\n        super().__init__()\n        ...\n\n    def call(self, ...):\n        ...\n\n    ...\n</code></pre></li> <li>Optionally, define your train and test datasets, these will be sharded by Maggy. <pre><code># Extract the data\n(x_train, y_train),(x_test, y_test) = split_dataset(dataset)\n\n# Do some preprocessing operations\n...\n</code></pre></li> <li> <p>Define a training function containing the training logic. <pre><code>def training_function(model, train_set, test_set, hparams):\n    #training and testing logic\n    ...\n</code></pre></p> </li> <li> <p>Create the configuration object and run the optimization. <pre><code>config = TfDistributedConfig(name=\"tf_test\", \n                             model=model, \n                             train_set=(x_train, y_train), \n                             test_set=(x_test, y_test),\n                             hparams=model_parameters),\n                             ...\n                             )\n\nexperiment.lagom(train_fn=training_function, config=config)\n</code></pre> There are many parameters for the configuration object:</p> <ul> <li>model: A tf.keras.Model superclass or list of them.        Note that this has to be the class itself, not an instance.</li> <li>train_set: The training set for the training function. If you want to load the set         inside the training function, this can be disregarded.</li> <li>test_set: The test set for the training function. If you want to load the set         inside the training function, this can be disregarded.</li> <li>process_data: The function for processing the data</li> <li>hparams: model parameters that should be used during model initialization. Primarily         used to give an interface for hp optimization.</li> <li>name: Experiment name.</li> <li>hb_interval: Heartbeat interval with which the server is polling.</li> <li>description: A description of the experiment.</li> </ul> </li> </ul>"},{"location":"dist_training/torch/","title":"Quick Start","text":"<p>Maggy enables you to train with Microsoft\u2019s DeepSpeed ZeRO optimizer. Since DeepSpeed does not follow the common  PyTorch programming model, Maggy is unable to provide full distribution transparency to the user.  This means that if you want to use DeepSpeed for your training, you will have to make small changes  to your code. In this notebook, we will show you what exactly you have to change in order to make  DeepSpeed run with Maggy.</p> <ul> <li> <p>First off, we have to define our model as we did for TensorFlow and Ablation studies. <pre><code>class MyModel(torch.nn.Module):\n\n    def __init__(self, ...):\n        super().__init__(...)\n        ...\n\n    def forward(self, ...):\n       ...\n</code></pre></p> </li> <li> <p>There are a few minor changes that have to be done in order to train with DeepSpeed: - There is no need for an  optimizer anymore. You can configure your optimizer later in the DeepSpeed config. - DeepSpeed\u2019s ZeRO requires you to  use FP16 training. Therefore, convert your data to half precision! - The backward call is not executed on the loss,  but on the model (<code>model.backward(loss)</code> instead of <code>loss.backward()</code>). -  The step call is not executed on the optimizer,  but also on the model (<code>model.step()</code> instead of <code>optimizer.step()</code>). -  As we have no optimizer anymore, there is also  no need to call <code>optimizer.zero_grad()</code>.  You do not have to worry about the implementation of these calls,  Maggy configures your model at runtime to act as a DeepSpeed engine. <pre><code>def train_fn(...):\n    ...\n</code></pre></p> </li> <li> <p>In order to use DeepSpeed\u2019s ZeRO, the deepspeed backend has to be chosen. This  backend also requires its own config. You can read a full specification of the possible settings  here. <pre><code>ds_config = {\"train_micro_batch_size_per_gpu\": 1,\n \"gradient_accumulation_steps\": 1,\n \"optimizer\": {\"type\": \"Adam\", \"params\": {\"lr\": 0.1}},\n \"fp16\": {\"enabled\": True},\n \"zero_optimization\": {\"stage\": 2},\n}\n\nconfig = TorchDistributedConfig(module=MyModel, backend=\"deepspeed\", deepspeed_config=ds_config, ...)\n</code></pre></p> </li> <li> <p>Start the training with <code>lagom()</code> <pre><code>result = experiment.lagom(train_fn, config)\n</code></pre></p> </li> </ul>"},{"location":"hpo/intro/","title":"Introduction","text":"<p>Maggy is a framework for asynchronous trials and early-stopping with global knowledge, guided by an Optimizer.  Developers can use an existing Optimizer, such as asynchronous successive halving (ASHA), or provide their own one.  The basic approach we followed was to add support for the Driver and Executors to communicate via RPCs.  The Optimizer that guides hyperparameter search is located on the Driver, and it assigns trials to Executors.  Executors periodically send back to the Driver the current performance of their trial,  and the Optimizer can decide to early-stop its ongoing trial, followed by sending the Executor with a new trial.  Because of the impedance mismatch between trials and the stage-/task-based execution model of Spark,  we are blocking Executors with long-running tasks to run multiple trials per task.  In this way, Executors are always kept busy running trials, and global information needed for efficient  early-stopping is aggregated in the Optimizer. If you want to know more about Maggy for Hyperparameter Optimization (HPO), you can watch the presentation in the video posted below. Otherwise, if you feel ready to explore more details, you can jump to the strategies section.</p>"},{"location":"hpo/intro/#sparkai-summit-presentation-of-maggy-for-hpo","title":"Spark/AI summit presentation of Maggy for HPO","text":""},{"location":"hpo/strategies/","title":"Quick Start","text":"<p>Using maggy for Hyperparameter Optimization (HPO) works as follows:</p> <ul> <li> <p>Define a training function containing the training logic. <pre><code>def training_function(model, train_set, test_set, hparams):\n    #training and testing logic\n    ...\n</code></pre></p> </li> <li> <p>Define a search space, containing the hparams we want to optimize, their type and range. <pre><code>#define the hyperparemeters to optimize, together with their possible values\nsp = Searchspace(kernel=('DISCRETE', [2, 8]), pool=('DISCRETE', [2, 8]), dropout=('DISCRETE', [0.01, 0.99]))\n</code></pre></p> </li> <li> <p>Create the configuration object and run the optimization. <pre><code>config = OptimizationConfig(num_trials=4, \n                            optimizer=\"randomsearch\", \n                            searchspace=sp, \n                            direction=\"max\", \n                            es_interval=1, \n                            es_min=5, \n                            name=\"hp_tuning_test\")\n\nexperiment.lagom(train_fn=training_function, config=config)\n</code></pre> There are many parameters for the configuration object:</p> <ul> <li>num_trials: Controls how many seperate runs are conducted during the hp search.</li> <li>optimizer: Optimizer type for searching the hp searchspace.</li> <li>searchspace: A Searchspace object configuring the names, types and ranges of hps.</li> <li>optimization_key: Name of the metric to use for hp search evaluation.</li> <li>direction: Direction of optimization.</li> <li>es_interval: Early stopping polling frequency during an experiment run.</li> <li>es_min: Minimum number of experiments to conduct before starting the early stopping mechanism. Useful to establish a baseline for performance estimates.</li> <li>es_policy: Early stopping policy which formulates a rule for triggering aborts.</li> <li>name: Experiment name.</li> <li>description: A description of the experiment.</li> <li>hb_interval: Heartbeat interval with which the server is polling.</li> <li>model: The class of the model to be used in the training function.</li> <li>train_set: The train_set to be used in the training function.</li> <li>test_set: The test_set to be used in the training function.</li> </ul> </li> </ul>"},{"location":"hpo/strategies/#strategies","title":"Strategies","text":""},{"location":"hpo/strategies/#random-search","title":"Random Search","text":"<p>With Random Search, the hparams are selected randomly within the search space defined. The search space is defined  depending on how many trials (num_trials) you choose. </p> <p>In the following example, num_trials is set to 4, therefore, Maggy will choose randomly 4 combinations of kernel,  pool and dropout values. <pre><code>def training_function(hparams):\n    #training and testing logic\n    ...\n#define the hyperparemeters to optimize, together with their possible values\nsp = Searchspace(kernel=('INTEGER', [2, 8]), pool=('INTEGER', [2, 8]), dropout=('DOUBLE', [0.01, 0.99]))\n\nconfig = OptimizationConfig(num_trials=4, \n                            optimizer=\"randomsearch\", \n                            searchspace=sp, \n                            direction=\"max\", \n                            es_interval=1, \n                            es_min=5, \n                            name=\"hp_tuning_test\")\n\n#run optimization\nresult = experiment.lagom(train_fn=training_function, config=config)\n</code></pre></p>"},{"location":"hpo/strategies/#grid-search","title":"Grid Search","text":"<pre><code>def training_function():\n    #training and testing logic\n    ...\n#define the hyperparemeters to optimize, together with their possible values\nsp = Searchspace(kernel=('INTEGER', [2, 8]), pool=('INTEGER', [2, 8]), dropout=('DOUBLE', [0.01, 0.99]))\n\nconfig = OptimizationConfig(num_trials=4, \n                            optimizer=\"gridsearch\", \n                            searchspace=sp, \n                            direction=\"max\", \n                            es_interval=1, \n                            es_min=5, \n                            name=\"hp_tuning_test\")\n\n#run optimization\nresult = experiment.lagom(train_fn=training_function, config=config)\n</code></pre>"},{"location":"hpo/strategies/#asynchronous-successive-halving-algorithm-asha","title":"Asynchronous Successive Halving Algorithm (ASHA)","text":"<p>This strategy is a combination of random search and early stopping.  ASHA tackles large-scale hyperparameter optimization problems, and it is especially useful for challenges that need a high number of parallelism (i.e. there are a lot of hparams and a lot of workers are available).</p> <pre><code>def training_function():\n    #training and testing logic\n    ...\n#define the hyperparemeters to optimize, together with their possible values\nsp = Searchspace(kernel=('INTEGER', [2, 8]), pool=('INTEGER', [2, 8]), dropout=('DOUBLE', [0.01, 0.99]))\n\nconfig = OptimizationConfig(num_trials=4, \n                            optimizer='asha', \n                            searchspace=sp, \n                            direction=\"max\", \n                            es_interval=1, \n                            es_min=5, \n                            name=\"hp_tuning_test\")\n\nexperiment.lagom(train_fn=training_function, config=config)\n</code></pre> <p>you can define custom ASHA optimizers by setting 3 parameters: reduction_factor, resource_min and resource_max. The standard values are 2, 1, and 4, respectively. To use custom values, import the class Asha from maggy.optimizer and create the object with custom  parameters.</p> <pre><code>from maggy.optimizer import Asha\n\nasha = Asha(3,1,10)\nconfig = OptimizationConfig(..., \n                            optimizer=asha, \n                            ...)\n</code></pre>"},{"location":"hpo/strategies/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>WIth Bayesian Optimization (BO), the hparams are chosen based on the space of the hparams.  In order to do that, the algorithm infer a function of the hparams in order to optimize the cost function of a given model.</p> <p>There are two different BO methods available in Maggy, namely Gaussian Process (GP) and Tree Parzen Estimators (TPE). The GP is a tool used to infer the value of a function in which predictions follow a normal distribution.  We use that set of predictions and pick new points where we should evaluate next. From that new point, we add it to  the samples and re-build the Gaussian Process with that new information\u2026  We keep doing this until we reach the maximal number of iterations or the limit time for example. TPE is an iterative process that uses history of evaluated hyperparameters to create probabilistic model,  which is used to suggest next set of hyperparameters to evaluate.</p> <pre><code>def training_function():\n    #training and testing logic\n    ...\n#define the hyperparemeters to optimize, together with their possible values\nsp = Searchspace(kernel=('INTEGER', [2, 8]), pool=('INTEGER', [2, 8]), dropout=('DOUBLE', [0.01, 0.99]))\n\nconfig = OptimizationConfig(num_trials=4, \n                            optimizer='gp', #or 'tpe' \n                            searchspace=sp, \n                            direction=\"max\", \n                            es_interval=1, \n                            es_min=5, \n                            name=\"hp_tuning_test\")\n\nexperiment.lagom(train_fn=training_function, config=config)\n</code></pre>"},{"location":"start/install/","title":"Installing Maggy in your laptop","text":"<p>Maggy is available via pip.</p> <p>Simply run the following commnad in your terminal or conda environment. </p> <pre><code>pip install maggy\n</code></pre> <p>If you want to use another version of Maggy, you can run the following command.</p> <pre><code>pip install maggy==x.y.z\n</code></pre> <p>The available versions are listed in PyPi https://pypi.org/project/maggy/ .</p>"},{"location":"start/install/#installing-maggy-in-hopsworks","title":"Installing Maggy in Hopsworks","text":"<p>If you are using Hopsworks, Maggy should be already installed and ready to be used.</p> <p>However, it is possible to check the installation from the platform by entering a project, then navigate to the \"Python\" section from the sidebar and click on \"Manage Environment\" on the top bar. Finally, search for \"Maggy\".</p> <p>If you want to change the version of Maggy, click on \"Install\" in the top bar and type \"Maggy\" on the search input. Finally, select the version you want to install and click. The progress of the installation is displayed in the \"Ongoing Operations\" section.</p> <p></p>"},{"location":"start/install/#installing-maggy-in-databricks","title":"Installing Maggy in Databricks","text":"<p>It is very simple to install Maggy in your Databricks cluster.  From your project, click on Libraries in the navigation bar and Install New, at this point it is possible to install  the latest release of Maggy in the PyPi section. In order to do that, just write \"maggy\" in the Package input section.</p> <p>You can install other version of Maggy by uploading the wheel on the Upload section.</p> <p></p>"},{"location":"start/quickstart/","title":"Quickstart","text":"<p>The programming model consists of writing a Python function that includes the training logic, that we call the training function.  Inside that training function provide all imports and parts that make up your experiment.</p>"},{"location":"start/quickstart/#what-can-i-do-with-maggy","title":"What can I do with Maggy?","text":"<p>Maggy can be used for four different ML experiments: Distributed Hyperparameter Optimization (HPO),  Distributed Ablation Study and Data Parallel Training and Model Parallel training (with DeepSpeed Zero),  their availability depends on which framework (either TensorFlow or PyTorch) and which platform (Hopsworks, Databricks, local) you are using:</p> Platform SparkAvailable DistributedHyperparameterOptimization DistributedAblation Study DistributedTraining* Hopsworks Yes Tensorflow Tensorflow Tensorflow, PyTorch Hopsworks No N.A. N.A. Tensorflow Databricks Yes Tensorflow Tensorflow Tensorflow, PyTorch Databricks No N.A. N.A. Tensorflow Local Yes N.A. N.A. Tensorflow, Pytorch Local No N.A. N.A. Tensorflow <p>*Distributed training with TensorFlow is Data Parallel training. Using PyTorch, Model Parallel Training is available.</p> <p>Spark is available automatically when you use Hopsworks and Databricks. Locally, it is easy to  install and set up.</p>"},{"location":"start/quickstart/#preparing-your-project-to-be-used-with-maggy","title":"Preparing your project to be used with Maggy","text":"<p>The usual workflow for a ML project will look something like this:</p> <pre><code>#datasets preparation\n...\n\n#model definition\n...\n\n#model training\n...\n\n#model testing\n...\n</code></pre> <p>With Maggy, the code will be like this:</p> <pre><code>#datasets preparation\n...\n\n#model definition\n...\n\ndef training_function(model, train_set, test_set, hparams):\n    #model training\n    ...\n\n    #model testing\n    ...\n\n#configure maggy, pass training_function to maggy and run the experiment\n...\n</code></pre> <p>There are three requirements for the training function:</p> <ol> <li>The function can take model, train_set, test_set, hparams as arguments (they are optionals),  plus one optional parameter reporter which is needed for reporting the current metric to the experiment driver.</li> <li>The function should return the metric that you want to optimize for.  This should coincide with the metric being reported in the Keras callback (see next point).</li> <li>In order to leverage on the early stopping capabilities of maggy,  you need to make use of the maggy reporter API. By including the reporter in your training loop,  you are telling maggy which metric to report back to the experiment driver for optimization and to check for global stopping.  It is as easy as adding reporter.broadcast(metric=YOUR_METRIC) for example at the end of your epoch or  batch training step and adding a reporter argument to your function signature. If you are not writing your own training loop you can use the pre-written Keras callbacks in the maggy.callbacks module.</li> </ol> <pre><code>def training_function(params):\n    #datasets preparation\n    ...\n\n    #model definition\n    ...\n\n    #model training\n    ...\n\n    #model testing\n    ...\n\n#configure maggy, pass training_function to maggy and run the experiment\n...\n</code></pre> <p>This case is the least efficient for Hyperparameter Optimization and Ablation Studies as it will process the data  and build a model for each execution. However, it won't affect Distributed Training.</p>"},{"location":"start/quickstart/#simple-example","title":"Simple Example","text":"<p>Colab Example</p> <p>In this example we are using Maggy for Data Parallel Training of a simple CNN model for the mnist dataset.</p> <p>First, we prepare the data and define the model, notice that we are not initializing it. That's because Maggy needs the class, not an instance of it. <pre><code>#datasets preparation\nmnist = tf.keras.datasets.mnist\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nx_train = np.reshape(x_train, (60000, 28, 28, 1))\nx_test = np.reshape(x_test, (10000, 28, 28, 1))\n\n#model definition\nclass SimpleCNN(tf.keras.Model):\n\n    def __init__(self, nlayers):\n        super().__init__()\n        self.conv1 = keras.layers.Conv2D(28, 2, activation='relu')\n        self.flatten = keras.layers.Flatten()\n        self.d1 = keras.layers.Dense(32, activation='relu')\n        self.d2 = keras.layers.Dense(10, activation='softmax')\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.flatten(x)\n        x = self.d1(x)\n        return self.d2(x)\n\nmodel = SimpleCNN #do not initialize the model, Maggy will do it for you\n</code></pre></p> <p>Now we wrap the traninig logic in a function that we called training_function. <pre><code>def training_function(model, train_set, test_set, hparams):\n    #model training\n\n    # Define training parameters\n    # Hyperparamters to optimize\n    nlayers = hparams['nlayers']\n    learning_rate = hparams['learning_rate']\n    # Fixed parameters\n    num_epochs = 10\n    batch_size = 256\n\n    criterion = keras.losses.SparseCategoricalCrossentropy()\n    optimizer = keras.optimizers.SGD(learning_rate=learning_rate,momentum=0.9,decay=1e-5)\n\n    model = model(nlayers = nlayers)\n\n    model.compile(optimizer=optimizer, loss=criterion, metrics=[\"accuracy\"])\n\n    #model training\n    model.fit(train_set[0],\n              train_set[1],\n              batch_size=batch_size,\n              epochs=num_epochs,\n              )\n\n    # model testing    \n    loss = model.evaluate(\n        test_set[0],\n        test_set[1],\n        batch_size=32)\n\n    return loss\n</code></pre></p> <p>Finally, we create a configuration class of the experiment we want to run (the configuration classes are Config,  TfDistributedConfig, TorchDistributedConfig, HyperparameterOptConfig and AblationConfig) and we launch  the experiment using the lagom function. (Lagom is a swedish word that stands for \"just the right amount\").</p> <pre><code>#configure maggy\nconfig = TfDistributedConfig(name=\"mnist_distributed_training\", \n                             model=model, \n                             train_set=(x_train, y_train), \n                             test_set=(x_test, y_test),\n                             hparams=model_parameters\n                            )\n\n#run the experiment\nloss = experiment.lagom(training_function, config)\n</code></pre> <p>For learning more about how to use Maggy for different use cases, you can navigate to  Hyperparameter Opitimization, Ablation Studies and  Training.</p>"}]}