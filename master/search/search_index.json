{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Maggy is a framework for distribution transparent machine learning experiments on Apache Spark . In this post, we introduce a new unified framework for writing core ML training logic as oblivious training functions . Maggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster. Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context. Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Quick Start # Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the 'spark' extra, to run on Spark in local mode: pip install maggy The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment. Single run experiment: def train_fn (): # This is your training iteration loop for i in range ( number_iterations ): ... # add the maggy reporter to report the metric to be optimized reporter . broadcast ( metric = accuracy ) ... # Return metric to be optimized or any metric to be logged return accuracy from maggy import experiment result = experiment . lagom ( train_fn = train_fn , name = 'MNIST' ) lagom is a Swedish word meaning \"just the right amount\". This is how MAggy uses your resources. Documentation # Full documentation is available at maggy.ai Contributing # There are various ways to contribute, and any contribution is welcome, please follow the CONTRIBUTING guide to get started. Issues # Issues can be reported on the official GitHub repo of Maggy. Citation # Please see our publications on maggy.ai to find out how to cite our work.","title":"Introduction"},{"location":"#quick-start","text":"Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the 'spark' extra, to run on Spark in local mode: pip install maggy The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment. Single run experiment: def train_fn (): # This is your training iteration loop for i in range ( number_iterations ): ... # add the maggy reporter to report the metric to be optimized reporter . broadcast ( metric = accuracy ) ... # Return metric to be optimized or any metric to be logged return accuracy from maggy import experiment result = experiment . lagom ( train_fn = train_fn , name = 'MNIST' ) lagom is a Swedish word meaning \"just the right amount\". This is how MAggy uses your resources.","title":"Quick Start"},{"location":"#documentation","text":"Full documentation is available at maggy.ai","title":"Documentation"},{"location":"#contributing","text":"There are various ways to contribute, and any contribution is welcome, please follow the CONTRIBUTING guide to get started.","title":"Contributing"},{"location":"#issues","text":"Issues can be reported on the official GitHub repo of Maggy.","title":"Issues"},{"location":"#citation","text":"Please see our publications on maggy.ai to find out how to cite our work.","title":"Citation"},{"location":"CONTRIBUTING/","text":"How to contribute # Contributions are welcome! Not familiar with the codebase yet? No problem! There are many ways to contribute to open source projects: reporting bugs, helping with the documentation, spreading the word and of course, adding new features and patches. Reporting issues # Describe what you expected to happen. If possible, include a minimal, complete, and verifiable example to help us identify the issue. This also helps to check that the issue is not with your own code. Describe what actually happened. Include the full traceback if there was an exception. List your Python, Hopsworks and Maggy versions. If possible, check if this issue is already fixed in the repository. Contributing Code # Code contributions, in the form of patches or features are welcome. In order to start developing, please follow the instructions below, to enable pre-commit and ensure style and codechecks. Python Setup # Fork Maggy to your GitHub account by clicking the Fork button. Clone your fork locally: git clone https://github.com/ [ username ] /maggy.git cd maggy Add the upstream repository as a remote to update later:: git remote add upstream https://github.com/logicalclocks/maggy.git git fetch upstream Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda: python3 -m venv env . env/bin/activate # or \"env\\Scripts\\activate\" on Windows or with conda: conda create --name maggy python = 3 .8 conda activate maggy verify your python version - we are using Python 3.8: python --version Install Maggy in editable mode with development dependencies:: pip install -e \".[dev]\" Install pre-commit_ and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Maggy uses pre-commit to ensure code-style and code formatting through black and flake8 : pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: flake8 maggy black maggy Start coding # Create a branch to identify the issue or feature you would like to work on. Using your favorite editor, make your changes, committing as you go. Follow PEP8 . Push your commits to GitHub and create a pull request . Celebrate \ud83c\udf89","title":"Contributing"},{"location":"CONTRIBUTING/#how-to-contribute","text":"Contributions are welcome! Not familiar with the codebase yet? No problem! There are many ways to contribute to open source projects: reporting bugs, helping with the documentation, spreading the word and of course, adding new features and patches.","title":"How to contribute"},{"location":"CONTRIBUTING/#reporting-issues","text":"Describe what you expected to happen. If possible, include a minimal, complete, and verifiable example to help us identify the issue. This also helps to check that the issue is not with your own code. Describe what actually happened. Include the full traceback if there was an exception. List your Python, Hopsworks and Maggy versions. If possible, check if this issue is already fixed in the repository.","title":"Reporting issues"},{"location":"CONTRIBUTING/#contributing-code","text":"Code contributions, in the form of patches or features are welcome. In order to start developing, please follow the instructions below, to enable pre-commit and ensure style and codechecks.","title":"Contributing Code"},{"location":"CONTRIBUTING/#python-setup","text":"Fork Maggy to your GitHub account by clicking the Fork button. Clone your fork locally: git clone https://github.com/ [ username ] /maggy.git cd maggy Add the upstream repository as a remote to update later:: git remote add upstream https://github.com/logicalclocks/maggy.git git fetch upstream Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda: python3 -m venv env . env/bin/activate # or \"env\\Scripts\\activate\" on Windows or with conda: conda create --name maggy python = 3 .8 conda activate maggy verify your python version - we are using Python 3.8: python --version Install Maggy in editable mode with development dependencies:: pip install -e \".[dev]\" Install pre-commit_ and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Maggy uses pre-commit to ensure code-style and code formatting through black and flake8 : pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: flake8 maggy black maggy","title":"Python Setup"},{"location":"CONTRIBUTING/#start-coding","text":"Create a branch to identify the issue or feature you would like to work on. Using your favorite editor, make your changes, committing as you go. Follow PEP8 . Push your commits to GitHub and create a pull request . Celebrate \ud83c\udf89","title":"Start coding"},{"location":"blogs/","text":"Blogs #","title":"Blogs"},{"location":"blogs/#blogs","text":"","title":"Blogs"},{"location":"publications/","text":"Publications # If you use Maggy for research, or write about Maggy please cite the following papers: Maggy Hyperparameter Optimization # Maggy: Scalable Asynchronous Parallel Hyperparameter Search # Authors # Moritz Meister, Sina Sheikholeslami, Amir H. Payberah, Vladimir Vlassov, Jim Dowling Abstract # Running extensive experiments is essential for building Machine Learning (ML) models. Such experiments usually require iterative execution of many trials with varying run times. In recent years, Apache Spark has become the de-facto standard for parallel data processing in the industry, in which iterative processes are im- plemented within the bulk-synchronous parallel (BSP) execution model. The BSP approach is also being used to parallelize ML trials in Spark. However, the BSP task synchronization barriers prevent asynchronous execution of trials, which leads to a reduced number of trials that can be run on a given computational budget. In this paper, we introduce Maggy, an open-source framework based on Spark, to execute ML trials asynchronously in parallel, with the ability to early stop poorly performing trials. In the experiments, we compare Maggy with the BSP execution of parallel trials in Spark and show that on random hyperparameter search on a con- volutional neural network for the Fashion-MNIST dataset Maggy reduces the required time to execute a fixed number of trials by 33% to 58%, without any loss in the final model accuracy. Download Paper Cite # @inproceedings{10.1145/3426745.3431338, author = {Meister, Moritz and Sheikholeslami, Sina and Payberah, Amir H. and Vlassov, Vladimir and Dowling, Jim}, title = {Maggy: Scalable Asynchronous Parallel Hyperparameter Search}, year = {2020}, isbn = {9781450381826}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3426745.3431338}, doi = {10.1145/3426745.3431338}, booktitle = {Proceedings of the 1st Workshop on Distributed Machine Learning}, pages = {28\u201333}, numpages = {6}, keywords = {Scalable Hyperparameter Search, Machine Learning, Asynchronous Hyperparameter Optimization}, location = {Barcelona, Spain}, series = {DistributedML'20} } Oblivious Training Functions # Towards Distribution Transparency for Supervised ML With Oblivious Training Functions # Authors # Moritz Meister, Sina Sheikholeslami, Robin Andersson, Alexandru A. Ormenisan, Jim Dowling Abstract # Building and productionizing Machine Learning (ML) models is a process of interdependent steps of iterative code updates, including exploratory model design, hyperparameter tuning, ablation experiments, and model training. Industrial-strength ML involves doing this at scale, using many compute resources, and this requires rewriting the training code to account for distribution. The result is that moving from a single host program to a cluster hinders iterative development of the software, as iterative development would require multiple versions of the software to be maintained and kept consistent. In this paper, we introduce the distribution oblivious training function as an abstraction for ML development in Python, whereby developers can reuse the same training function when running a notebook on a laptop or performing scale-out hyperparameter search and distributed training on clusters. Programs written in our framework look like industry-standard ML programs as we factor out dependencies using best-practice programming idioms (such as functions to generate models and data batches). We believe that our approach takes a step towards unifying single-host and distributed ML development. Download Paper Cite # @inproceedings{oblivious-mlops, author = {Meister, Moritz and Sheikholeslami, Sina and Andersson, Robin and Ormenisan, Alexandru A. and Dowling, Jim}, title = {Towards Distribution Transparency for Supervised ML With Oblivious Training Functions}, year = {2020}, booktitle = {MLSys \u201920: Workshop on MLOps Systems, March 02\u201304}, location = {Austin, Texas, USA} }","title":"Publications"},{"location":"publications/#publications","text":"If you use Maggy for research, or write about Maggy please cite the following papers:","title":"Publications"},{"location":"publications/#maggy-hyperparameter-optimization","text":"","title":"Maggy Hyperparameter Optimization"},{"location":"publications/#maggy-scalable-asynchronous-parallel-hyperparameter-search","text":"","title":"Maggy: Scalable Asynchronous Parallel Hyperparameter Search"},{"location":"publications/#oblivious-training-functions","text":"","title":"Oblivious Training Functions"},{"location":"publications/#towards-distribution-transparency-for-supervised-ml-with-oblivious-training-functions","text":"","title":"Towards Distribution Transparency for Supervised ML With Oblivious Training Functions"},{"location":"releases/","text":"","title":"Releases"},{"location":"ablation/intro/","text":"","title":"Introduction"},{"location":"dist_training/intro/","text":"","title":"Introduction"},{"location":"hpo/intro/","text":"","title":"Introduction"},{"location":"start/install/","text":"Installation #","title":"Installation"},{"location":"start/install/#installation","text":"","title":"Installation"},{"location":"start/quickstart/","text":"Quickstart #","title":"Quickstart"},{"location":"start/quickstart/#quickstart","text":"","title":"Quickstart"}]}